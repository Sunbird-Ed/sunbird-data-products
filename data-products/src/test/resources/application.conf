application.env="local"
telemetry.version="2.1"
default.parallelization=10
spark_output_temp_dir="/tmp/"

# the below urls should be https - do not update them
lp.url="https://dev.ekstep.in/api/learning"
lp.path="/api/learning/v2/content/"

default.consumption.app.id="genie"
default.channel.id="in.sunbird"
default.creation.app.id="portal"

lp.contentmodel.versionkey=jd5ECm/o0BXwQCe8PfZY1NoUkB9HN41QjA80p22MKyRIcP5RW4qHw8sZztCzv87M

# Test Configurations
cassandra.service.embedded.enable=true
cassandra.cql_path="src/main/resources/data.cql"
cassandra.service.embedded.connection.port=9142
cassandra.keyspace_prefix="local_"
cassandra.hierarchy_store_prefix="dev_"

spark.cassandra.connection.host=127.0.0.1

# Slack Configurations
monitor.notification.channel = "testing"
monitor.notification.name = "dp-monitor"
monitor.notification.slack = false

# DataExhaust configuration
data_exhaust {
	save_config {
		save_type="local"
		bucket="ekstep-dev-data-store"
		prefix="/tmp/data-exhaust/"
		public_s3_url="s3://"
		local_path="/tmp/data-exhaust-package/"
	}
	delete_source: "false"
	package.enable: "false"
}

cloud_storage_type="local"

service.search.url="https://dev.sunbirded.org/api/content"
service.search.path="/v1/search"

hierarchy.search.api.url="https://dev.sunbirded.org/api/course"
hierarchy.search.api.path="/v1/hierarchy/"

#service.search.url="http://11.2.4.16:9000"

elasticsearch.service.endpoint="http://localhost:9200"
elasticsearch.index.compositesearch.name="compositesearch"

## Reports - Global config
cloud.container.reports="reports"

# course metrics container in azure
es.host="http://localhost"
es.composite.host="localhost"
es.port="9200"
course.metrics.cassandra.sunbirdKeyspace="sunbird"
course.metrics.cassandra.sunbirdCoursesKeyspace="sunbird_courses"
course.metrics.cassandra.sunbirdHierarchyStore="dev_hierarchy_store"
admin.reports.cloud.container="reports"
# Folder names within container
course.metrics.cloud.objectKey="src/test/resources/reports/"
admin.metrics.cloud.objectKey="src/test/resources/admin-user-reports/"

# End of report folder names
course.metrics.cassandra.input.consistency="QUORUM"
assessment.metrics.cassandra.input.consistency="QUORUM"
assessment.metrics.bestscore.report=true // BestScore or Latst Updated Score
assessment.metrics.supported.contenttype="SelfAssess"
assessment.metrics.supported.primaryCategories="Practice Question Set"
spark.sql.caseSensitive=true

reports_storage_key="test"
reports_storage_secret="test"
# for only testing uploads to blob store
azure_storage_key=""
azure_storage_secret=""

# content rating configurations
druid.sql.host="http://localhost:8082/druid/v2/sql/"
druid.unique.content.query="{\"query\":\"SELECT DISTINCT \\\"object_id\\\" AS \\\"Id\\\"\\nFROM \\\"druid\\\".\\\"summary-events\\\" WHERE \\\"__time\\\"  BETWEEN TIMESTAMP '%s' AND TIMESTAMP '%s'\"}"
druid.content.rating.query="{\"query\":\"SELECT \\\"object_id\\\" AS contentId, COUNT(*) AS \\\"totalRatingsCount\\\", SUM(edata_rating) AS \\\"Total Ratings\\\", SUM(edata_rating)/COUNT(*) AS \\\"averageRating\\\" FROM \\\"druid\\\".\\\"telemetry-feedback-events\\\" WHERE \\\"eid\\\" = 'FEEDBACK' GROUP BY \\\"object_id\\\"\"}"
lp.system.update.base.url="http://11.2.4.22:8080/learning-service/system/v3/content/update"

druid = {
	hosts = "localhost:8082"
	secure = false
	url = "/druid/v2/"
	datasource = "summary-events"
	response-parsing-timeout = 300000
}
druid.query.wait.time.mins=1
druid.report.upload.wait.time.mins=1
druid.scan.batch.size=100
druid.scan.batch.bytes=2000000
druid.query.batch.buffer=2

#Experiment Configuration
user.search.api.url = "http://localhost:9000/private/user/v1/search"
user.search.limit = 10000

spark.memory_fraction=0.3
spark.storage_fraction=0.5
spark.driver_memory=1g

// Metric event config
metric.producer.id="pipeline.monitoring"
metric.producer.pid="dataproduct.metrics"
push.metrics.kafka=false
metric.kafka.broker="localhost:9092"
metric.kafka.topic="metric"

//Postgres Config
postgres.db="postgres"
postgres.url="jdbc:postgresql://localhost:65124/"
postgres.user="postgres"
postgres.pass="postgres"
postgres.program.table="program"
postgres.nomination.table="nomination"
postgres.usertable="\"V_User\""
postgres.org.table="\"V_User_Org\""

druid.ingestion.path="/druid/indexer/v1/task"
druid.segment.path="/druid/coordinator/v1/metadata/datasources/"
druid.deletesegment.path="/druid/coordinator/v1/datasources/"
druid.content.consumption.query="{\"query\":\"SELECT COUNT(*) as \\\"play_sessions_count\\\", SUM(total_time_spent) as \\\"total_time_spent\\\", dimensions_pdata_id, object_id\\nFROM \\\"summary-events\\\"\\nWHERE \\\"dimensions_mode\\\" = 'play' AND \\\"dimensions_type\\\" ='content'\\nGROUP BY object_id, dimensions_pdata_id\"}"
// TPD Configurations
org.search.api.url="https://dev.sunbirded.org/api"
org.search.api.path="/org/v1/search"
druid.host="http://localhost:8082/druid/v2"
elasticsearch.index.coursebatch.name="course-batch"

sunbird_encryption_key="SunBird"
sunbird_encryption="ON"
dcedialcode.filename="DCE_dialcode_data.csv"
etbdialcode.filename="ETB_dialcode_data.csv"
dcetextbook.filename="DCE_textbook_data.csv"
etbtextbook.filename="ETB_textbook_data.csv"
etb.dialcode.druid.length=20

//redis connection
redis.connection.max=2
redis.connection.idle.max=2
redis.connection.idle.min=1
redis.connection.minEvictableIdleTimeSeconds=120
redis.connection.timeBetweenEvictionRunsSeconds=300
redis.user.index=12
collection.exhaust.store.prefix="src/test/resources/exhaust-reports"

## Collection Exhaust Jobs Configuration - Start ##

sunbird.user.keyspace="sunbird"
sunbird.courses.keyspace="sunbird_courses"
sunbird.user.keyspace="sunbird"
sunbird.content.hierarchy.keyspace="dev_hierarchy_store"
sunbird.user.cluster.host=127.0.0.1
sunbird.courses.cluster.host=127.0.0.1
sunbird.content.cluster.host=127.0.0.1
sunbird.report.cluster.host=127.0.0.1
sunbird.user.report.keyspace="sunbird_courses"
collection.exhaust.store.prefix="reports"
postgres.table.job_request="job_request"

druid.report.default.storage="local"
druid.report.date.format="yyyy-MM-dd"

exhaust.batches.limit.per.channel=3
// file size in bytes
exhaust.file.size.limit=100

exhaust.parallel.batch.load.limit = 10
exhaust.user.parallelism=200
exhaust.file.size.limit.per.channel=100
data_exhaust.batch.limit.per.request=4



//START of UCI Postgres Config
uci.conversation.postgres.db="postgres"
uci.conversation.postgres.url="jdbc:postgresql://localhost:65124/"

uci.fushionauth.postgres.db="postgres"
uci.fushionauth.postgres.url="jdbc:postgresql://localhost:65124/"

uci.postgres.table.conversation="bot"
uci.postgres.table.user="users"
uci.postgres.table.user_registration="user_registrations"
uci.postgres.table.identities="identities"

uci.fushionauth.postgres.user="postgres"
uci.fushionauth.postgres.pass="postgres"

uci.conversation.postgres.user="postgres"
uci.conversation.postgres.pass="postgres"
uci.exhaust.store.prefix="src/test/resources/exhaust-reports/"
uci.encryption.secret="123443957398423479784298247982789428fldkssd"
// END OF UCI Related Job Configs
